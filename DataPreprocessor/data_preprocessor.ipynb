{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-06-01T10:24:20.193328Z",
     "start_time": "2025-06-01T10:24:18.011022Z"
    }
   },
   "source": [
    "import time\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from collections import Counter\n",
    "# Hämta nuvarande datum och tid\n",
    "now = datetime.datetime.now()\n",
    "# Formatera som strängq\n",
    "timestamp = now.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "class SurveyDataProcessor:\n",
    "    \"\"\"Processes survey data with ML-based imputation for missing values.\"\"\"\n",
    "\n",
    "    def __init__(self, filepath):\n",
    "        \"\"\"Initialize with survey data filepath.\"\"\"\n",
    "        self.filepath = filepath\n",
    "        self.df_original = None\n",
    "        self.df_clean = None\n",
    "        self.models = {}\n",
    "        self.scalers = {}\n",
    "        self.feature_sets = {}\n",
    "        self.imputation_log = []\n",
    "\n",
    "    def load_and_prepare_data(self):\n",
    "        \"\"\"Load data and apply column mappings.\"\"\"\n",
    "        print('Loading and preparing survey data...')\n",
    "\n",
    "        # Load data\n",
    "        self.df_original = pd.read_excel(self.filepath)\n",
    "\n",
    "        # Rename columns\n",
    "        self._rename_columns()\n",
    "\n",
    "        # Apply categorical mappings\n",
    "        self._apply_categorical_mappings()\n",
    "\n",
    "        # Create composite variables\n",
    "        self._create_composite_variables()\n",
    "\n",
    "        print(f\"Data loaded: {len(self.df_original)} rows\")\n",
    "\n",
    "    def _rename_columns(self):\n",
    "        \"\"\"Rename columns to shorter, standardized names.\"\"\"\n",
    "        column_mapping = {\n",
    "            'Tidstämpel': 'TIME',\n",
    "            'Kön': 'GDR',\n",
    "            'Åldersgrupp': 'AGE',\n",
    "            'Län': 'COUNTY',\n",
    "            'Vilken/vilka språkmodell(er) använder du idag i ditt arbete?': 'UB1',\n",
    "            'När var första gången du använde en språkmodell i ditt arbete?': 'EXP',\n",
    "            'Hur ofta har du använt språkmodeller i ditt arbete under det senaste året?': 'UB2',\n",
    "            'Jag upplever att språkmodeller är i allmänhet användbara i mitt arbete som lärare.': 'PE1',\n",
    "            'Språkmodeller hjälper mig att planera och förbereda min undervisning snabbare än traditionella metoder.': 'PE2',\n",
    "            'Genom att använda språkmodeller kan jag öka antalet eller variationen av undervisningsmaterial jag producerar.': 'PE3',\n",
    "            'Språkmodeller förbättrar kvaliteten på mitt arbete genom att hjälpa mig med administrativa och rutinmässiga arbetsuppgifter.': 'PE4',\n",
    "            'Det krävs lite ansträngning för mig att bli kompetent i att använda språkmodeller för läraruppgifter.': 'EE1',\n",
    "            'Språkmodeller är/skulle vara intuitiva och okomplicerade att använda i mitt dagliga arbete som lärare.': 'EE2',\n",
    "            'Inlärningsprocessen för att komma igång med språkmodeller kräver/skulle kräva minimal ansträngning från min sida.': 'EE3',\n",
    "            'Personer i mitt privatliv (familj och vänner) anser att jag bör använda stora språkmodeller i mitt arbete som lärare.': 'SI1',\n",
    "            'Mina ämneskollegor och andra lärare på skolan stödjer/uppmuntrar till att använda språkmodeller i undervisningen.': 'SI2',\n",
    "            'Skolledningen (rektor och ledningsgrupp) stödjer/uppmuntrar aktivt användningen av språkmodeller i undervisningen.': 'SI3',\n",
    "            'Jag har tillgång till nödvändig teknisk utrustning (dator, internetuppkoppling, etc.) för att effektivt använda stora språkmodeller i min undervisning.': 'FC1',\n",
    "            'Jag har tillräcklig kunskap om språkmodeller för att kunna använda de på ett effektivt sätt.': 'FC2',\n",
    "            'Språkmodeller fungerar väl tillsammans med de digitala lärplattformar och andra verktyg jag redan använder.': 'FC3',\n",
    "            'Det finns tillgång till specialiserad IT-support eller kollegor med expertkunskap som kan hjälpa mig när jag stöter på problem med språkmodeller.': 'FC4',\n",
    "            'Skolledningen har uttryckligen förmedlat en förväntan om att jag ska integrera språkmodeller i min undervisningspraktik.': 'VOL1',\n",
    "            'Jag har frihet att själv bestämma i vilken utsträckning och för vilka syften jag använder språkmodeller.': 'VOL2',\n",
    "            'Användning av språkmodeller är ett formellt krav i min tjänst och ingår i skolans officiella riktlinjer för digitala verktyg.': 'VOL3',\n",
    "            'Inom vilken tidsram förväntar du dig att börja använda/öka din användning av språkmodeller i ditt arbete?': 'BI1',\n",
    "            'Baserat på mina erfarenheter hittills, räknar jag med att språkmodeller kommer bli ett regelbundet verktyg i min undervisning framöver.': 'BI2',\n",
    "            'Jag ser långsiktiga möjligheter att integrera språkmodeller i min professionella utveckling som lärare och i utformningen av mina kurser.': 'BI3',\n",
    "            'Tillåter du att dina elever använder språkmodeller (som ChatGPT) i sitt skolarbete?': 'STU'\n",
    "        }\n",
    "        self.df_original = self.df_original.rename(columns=column_mapping)\n",
    "\n",
    "    def _apply_categorical_mappings(self):\n",
    "        \"\"\"Apply categorical variable mappings.\"\"\"\n",
    "        # County mapping\n",
    "        county_mapping = {\n",
    "            'Blekinge län': 0, 'Dalarnas län': 1, 'Gotlands län': 2, 'Gävleborgs län': 3,\n",
    "            'Hallands län': 4, 'Jämtlands län': 5, 'Jönköpings län': 6, 'Kalmar län': 7,\n",
    "            'Kronobergs län': 8, 'Norrbottens län': 9, 'Örebro län': 10, 'Östergötlands län': 11,\n",
    "            'Stockholms län': 12, 'Södermanlands län': 13, 'Skåne län': 14, 'Värmlands län': 15,\n",
    "            'Västerbottens län': 16, 'Västmanlands län': 17, 'Västra Götalands län': 18,\n",
    "            'Uppsala län': 19, 'Västernorrlands län': 20\n",
    "        }\n",
    "\n",
    "        # Age mapping\n",
    "        age_mapping = {\n",
    "            '25-29 år': 0, '30-34 år': 1, '35-39 år': 2, '40-44 år': 3,\n",
    "            '45-49 år': 4, '50-54 år': 5, '55-59 år': 6, '60-64 år': 7, '65 år eller äldre': 8\n",
    "        }\n",
    "\n",
    "        # Experience mapping\n",
    "        exp_mapping = {\n",
    "            'Jag har aldrig använt en språkmodell i mitt arbete': 0,\n",
    "            'För mindre än 1 månad sen': 1, 'För 1–3 månader sen': 2, 'För 4–12 månader sen': 3,\n",
    "            'För 1–2 år sen': 4, 'För mer än 2 år sen': 5\n",
    "        }\n",
    "\n",
    "        # Usage frequency mapping\n",
    "        ub2_mapping = {\n",
    "            'Aldrig': 0, 'Mer sällan än en gång i månaden': 1, 'Någon gång i månaden': 2,\n",
    "            'Några gånger i veckan': 3, 'Dagligen': 4\n",
    "        }\n",
    "\n",
    "        # Behavioral intention mapping\n",
    "        bi1_mapping = {\n",
    "            'Aldrig': 0, 'Inte inom de fem kommande åren': 1,\n",
    "            'Inom två till fem år': 2, 'Inom ett år': 3,\n",
    "            'Inom de sex kommande månaderna': 4, 'Jag använder redan språkmodeller': 5\n",
    "        }\n",
    "\n",
    "        # Apply mappings\n",
    "        self.df_original['COUNTY'] = self.df_original['COUNTY'].map(county_mapping)\n",
    "        self.df_original['AGE'] = self.df_original['AGE'].map(age_mapping)\n",
    "        self.df_original['EXP'] = self.df_original['EXP'].map(exp_mapping)\n",
    "        self.df_original['UB2'] = self.df_original['UB2'].map(ub2_mapping)\n",
    "        self.df_original['BI1'] = self.df_original['BI1'].map(bi1_mapping)\n",
    "        self.df_original['STU'] = self.df_original['STU'].map({'Nej': 0, 'Ja': 1})\n",
    "        self.df_original['UB1'] = self.df_original['UB1'].apply(\n",
    "            lambda x: 1 if 'Använder ej' not in str(x) else 0\n",
    "        )\n",
    "\n",
    "    def _create_composite_variables(self):\n",
    "        \"\"\"Create composite variables from individual items.\"\"\"\n",
    "        # Invert coercive variables\n",
    "        self.df_original['VOL1'] = 8 - self.df_original['VOL1']\n",
    "        self.df_original['VOL3'] = 8 - self.df_original['VOL3']\n",
    "\n",
    "        # Create composite scores\n",
    "        self.df_original['VOL'] = self.df_original[['VOL1', 'VOL3']].mean(axis=1)\n",
    "        self.df_original['PE'] = self.df_original[['PE1', 'PE2', 'PE3', 'PE4']].mean(axis=1)\n",
    "        self.df_original['EE'] = self.df_original[['EE2', 'EE3']].mean(axis=1)\n",
    "        self.df_original['SI'] = self.df_original[['SI1', 'SI2', 'SI3']].mean(axis=1)\n",
    "        self.df_original['FC'] = self.df_original[['FC1', 'FC2', 'FC3']].mean(axis=1)\n",
    "        self.df_original['BI'] = self.df_original[['BI1', 'BI2', 'BI3']].mean(axis=1)\n",
    "\n",
    "        # Normalize BI to 0-1 scale\n",
    "        bi_min, bi_max = self.df_original['BI'].min(), self.df_original['BI'].max()\n",
    "        self.df_original['BI'] = (self.df_original['BI'] - bi_min) / (bi_max - bi_min)\n",
    "\n",
    "        self.df_original['UB'] = self.df_original[['UB1', 'UB2']].mean(axis=1)\n",
    "\n",
    "    def analyze_missing_values(self):\n",
    "        \"\"\"Analyze and report missing values in the dataset.\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"MISSING VALUES ANALYSIS\")\n",
    "        print(\"=\"*60)\n",
    "\n",
    "        # Find rows with missing values\n",
    "        nan_rows = self.df_original[self.df_original.isna().any(axis=1)]\n",
    "        print(f'Total rows with missing values: {len(nan_rows)}')\n",
    "\n",
    "        if len(nan_rows) > 0:\n",
    "            print('\\nDetails of rows with missing values:')\n",
    "            for index, row in nan_rows.iterrows():\n",
    "                nan_cols = row[row.isna()].index.tolist()\n",
    "                print(f\"Row {index}: missing values in columns {nan_cols}\")\n",
    "\n",
    "        # Gender distribution\n",
    "        gender_count = self.df_original['GDR'].value_counts(dropna=False)\n",
    "        print(f'\\nGender distribution:')\n",
    "        for category, count in gender_count.items():\n",
    "            print(f\"  {category}: {count}\")\n",
    "\n",
    "    def prepare_training_data(self):\n",
    "        \"\"\"Prepare clean dataset for model training.\"\"\"\n",
    "        # Remove unnecessary columns and clean data\n",
    "        self.df_clean = self.df_original.drop(['TIME', 'EE1', 'FC4', 'VOL2'], axis=1)\n",
    "        self.df_clean = self.df_clean[self.df_clean['GDR'] != 'Annat']\n",
    "        self.df_clean['GDR'] = self.df_clean['GDR'].map({'Kvinna': 0, 'Man': 1})\n",
    "        self.df_clean = self.df_clean.dropna()\n",
    "        self.df_clean.to_excel(f'{timestamp}_pure-data.xlsx', index=False)\n",
    "        print(f\"Clean dataset for training: {len(self.df_clean)} rows\")\n",
    "\n",
    "    def train_model(self, target_column, label_names):\n",
    "        \"\"\"Train a Random Forest model for the specified target.\"\"\"\n",
    "        print(f\"\\nTraining model for {target_column}...\")\n",
    "\n",
    "        # Prepare features (exclude target and composite variables)\n",
    "        composite_vars = ['PE', 'EE', 'SI', 'FC', 'BI', 'UB']\n",
    "        X = self.df_clean.drop(columns=[target_column] + composite_vars)\n",
    "        y = self.df_clean[target_column]\n",
    "\n",
    "        # Feature selection using Random Forest importance\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "        rf_selector = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "        rf_selector.fit(X_scaled, y)\n",
    "\n",
    "        # Select top 8 features\n",
    "        importance_df = pd.DataFrame({\n",
    "            'Feature': X.columns,\n",
    "            'Importance': rf_selector.feature_importances_\n",
    "        })\n",
    "        top_features = importance_df.sort_values('Importance', ascending=False).head(8)['Feature'].tolist()\n",
    "\n",
    "        # Train final model with selected features\n",
    "        X_selected = self.df_clean[top_features]\n",
    "        X_selected_scaled = scaler.fit_transform(X_selected)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X_selected_scaled, y, test_size=0.2, random_state=42\n",
    "        )\n",
    "\n",
    "        model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Evaluate model\n",
    "        y_pred = model.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        print(f\"Model accuracy: {accuracy:.3f}\")\n",
    "\n",
    "        # Store model artifacts\n",
    "        self.models[target_column] = model\n",
    "        self.scalers[target_column] = scaler\n",
    "        self.feature_sets[target_column] = top_features\n",
    "\n",
    "        # Generate confusion matrix visualization\n",
    "        self._create_confusion_matrix(y_test, y_pred, target_column, label_names)\n",
    "\n",
    "        return model, scaler, top_features\n",
    "\n",
    "    def _create_confusion_matrix(self, y_test, y_pred, target_name, label_names):\n",
    "        \"\"\"Create and save confusion matrix visualization.\"\"\"\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "        # Calculate performance metrics\n",
    "        tn, fp, fn, tp = cm.ravel() if cm.shape == (2, 2) else (cm[0,0], cm[0,1], cm[1,0], cm[1,1])\n",
    "        accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "        # Set academic formatting\n",
    "        plt.rcParams.update({\n",
    "            'font.family': 'serif',\n",
    "            'font.serif': ['Times New Roman'],\n",
    "            'font.size': 12,\n",
    "            'axes.linewidth': 1.0,\n",
    "            'axes.edgecolor': 'black'\n",
    "        })\n",
    "\n",
    "        # Create confusion matrix table\n",
    "        fig, ax = plt.subplots(figsize=(8, 6))\n",
    "        ax.axis('tight')\n",
    "        ax.axis('off')\n",
    "\n",
    "        table_data = [\n",
    "            ['', 'Predikterad klass', ''],\n",
    "            ['', label_names[0], label_names[1]],\n",
    "            [f'Verklig klass', '', ''],\n",
    "            [label_names[0], str(cm[0,0]), str(cm[0,1])],\n",
    "            [label_names[1], str(cm[1,0]), str(cm[1,1])]\n",
    "        ]\n",
    "\n",
    "        table = ax.table(cellText=table_data, loc='center', cellLoc='center')\n",
    "        table.auto_set_font_size(False)\n",
    "        table.set_fontsize(12)\n",
    "        table.scale(1.2, 2)\n",
    "\n",
    "        # Format table cells\n",
    "        for i in range(len(table_data)):\n",
    "            for j in range(len(table_data[0])):\n",
    "                cell = table[(i, j)]\n",
    "                if i == 0 or (i == 2 and j == 0):\n",
    "                    cell.set_text_props(weight='bold', style='italic')\n",
    "                elif i == 1 or (i > 2 and j == 0):\n",
    "                    cell.set_text_props(weight='bold')\n",
    "\n",
    "                if i == 0:\n",
    "                    cell.set_linewidth(2)\n",
    "                elif i == 2:\n",
    "                    cell.set_linewidth(1.5)\n",
    "                else:\n",
    "                    cell.set_linewidth(1)\n",
    "\n",
    "        # Add title and metrics\n",
    "        target_display = 'STU-klassificering' if target_name == 'STU' else 'Könsklassificering'\n",
    "        table_num = 1 if target_name == 'STU' else 2\n",
    "        plt.suptitle(f'Tabell {table_num}. Konfusionsmatris för {target_display}',\n",
    "                     fontsize=12, fontweight='bold', y=0.85)\n",
    "\n",
    "        metrics_text = f\"\"\"Klassificeringsprestanda:\n",
    "Noggrannhet: {accuracy:.3f} ({tp + tn}/{tp + tn + fp + fn})\n",
    "Precision: {precision:.3f} ({tp}/{tp + fp})\n",
    "Återkallelse: {recall:.3f} ({tp}/{tp + fn})\n",
    "F1-Poäng: {f1:.3f}\n",
    "Specificitet: {specificity:.3f} ({tn}/{tn + fp})\n",
    "\n",
    "Not: Värdena representerar antalet prover klassificerade i varje kategori\n",
    "(n = {len(y_test)} testprover).\"\"\"\n",
    "\n",
    "        plt.figtext(0.1, 0.3, metrics_text, fontsize=10, verticalalignment='top',\n",
    "                    bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightgray\", alpha=0.3))\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.subplots_adjust(bottom=0.4)\n",
    "\n",
    "        filename = f'Tabell_{table_num}_Konfusionsmatris_{target_name}.png'\n",
    "        plt.savefig(filename, dpi=300, bbox_inches='tight', facecolor='white')\n",
    "        plt.close()\n",
    "\n",
    "        print(f\"Confusion matrix saved as: {filename}\")\n",
    "\n",
    "    def perform_imputation(self):\n",
    "        \"\"\"Perform detailed imputation tracking for missing values.\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"DETAILED IMPUTATION TRACKING\")\n",
    "        print(\"=\"*60)\n",
    "\n",
    "        df_imputation = self.df_original.copy()\n",
    "        self.imputation_log = []\n",
    "\n",
    "        # Initialize imputation tracking columns\n",
    "        self._initialize_imputation_columns(df_imputation)\n",
    "\n",
    "        # STU imputation\n",
    "        self._impute_stu_values(df_imputation)\n",
    "\n",
    "        # GDR imputation\n",
    "        self._impute_gdr_values(df_imputation)\n",
    "\n",
    "        # Save results\n",
    "        self._save_imputation_results(df_imputation)\n",
    "\n",
    "        return df_imputation\n",
    "\n",
    "    def _initialize_imputation_columns(self, df):\n",
    "        \"\"\"Initialize columns to track which values were imputed.\"\"\"\n",
    "        # Create imputation tracking columns for each variable that might be imputed\n",
    "        df['STU_IMPUTED'] = 0  # 0 = original, 1 = imputed\n",
    "        df['GDR_IMPUTED'] = 0  # 0 = original, 1 = imputed\n",
    "\n",
    "        # Mark rows where original data was missing or 'Annat'\n",
    "        df.loc[df['STU'].isnull(), 'STU_IMPUTED'] = 1\n",
    "        df.loc[df['GDR'] == 'Annat', 'GDR_IMPUTED'] = 1\n",
    "\n",
    "        print(f\"Initialized imputation tracking columns:\")\n",
    "        print(f\"  STU values to be imputed: {df['STU_IMPUTED'].sum()}\")\n",
    "        print(f\"  GDR values to be imputed: {df['GDR_IMPUTED'].sum()}\")\n",
    "\n",
    "    def _impute_stu_values(self, df):\n",
    "        \"\"\"Impute missing STU values using trained model.\"\"\"\n",
    "        print(f\"\\nSTU IMPUTATION:\")\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "        stu_missing_mask = df['STU'].isnull()\n",
    "        stu_missing_indices = df[stu_missing_mask].index.tolist()\n",
    "\n",
    "        print(f\"Rows with missing STU values: {len(stu_missing_indices)}\")\n",
    "\n",
    "        if len(stu_missing_indices) == 0:\n",
    "            print(\"No STU values need imputation\")\n",
    "            return\n",
    "\n",
    "        if 'STU' not in self.models:\n",
    "            print(\"STU model not trained. Skipping STU imputation.\")\n",
    "            return\n",
    "\n",
    "        try:\n",
    "            df_missing = df[stu_missing_mask]\n",
    "            features = self.feature_sets['STU']\n",
    "            X_missing = df_missing[features]\n",
    "\n",
    "            # Check for NaN values in required features\n",
    "            if X_missing.isnull().any().any():\n",
    "                print(\"Warning: NaN values in required features for STU prediction\")\n",
    "                valid_mask = ~X_missing.isnull().any(axis=1)\n",
    "                X_valid = X_missing[valid_mask]\n",
    "                valid_indices = X_valid.index.tolist()\n",
    "\n",
    "                if len(valid_indices) > 0:\n",
    "                    X_scaled = self.scalers['STU'].transform(X_valid)\n",
    "                    predictions = self.models['STU'].predict(X_scaled)\n",
    "\n",
    "                    for i, idx in enumerate(valid_indices):\n",
    "                        old_value = df.loc[idx, 'STU']\n",
    "                        new_value = predictions[i]\n",
    "                        df.loc[idx, 'STU'] = new_value\n",
    "\n",
    "                        self._log_imputation(idx, 'STU', old_value, new_value, 'Model prediction')\n",
    "                        print(f\"  Row {idx}: STU imputed from {old_value} to {new_value}\")\n",
    "            else:\n",
    "                X_scaled = self.scalers['STU'].transform(X_missing)\n",
    "                predictions = self.models['STU'].predict(X_scaled)\n",
    "\n",
    "                for i, idx in enumerate(stu_missing_indices):\n",
    "                    old_value = df.loc[idx, 'STU']\n",
    "                    new_value = predictions[i]\n",
    "                    df.loc[idx, 'STU'] = new_value\n",
    "\n",
    "                    self._log_imputation(idx, 'STU', old_value, new_value, 'Model prediction')\n",
    "                    print(f\"  Row {idx}: STU imputed from {old_value} to {new_value}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error in STU imputation: {e}\")\n",
    "\n",
    "    def _impute_gdr_values(self, df):\n",
    "        \"\"\"Impute GDR values marked as 'Annat' using trained model.\"\"\"\n",
    "        print(f\"\\n\\nGDR IMPUTATION WITH RANDOM FOREST:\")\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "        gdr_annat_mask = df['GDR'] == 'Annat'\n",
    "        gdr_annat_indices = df[gdr_annat_mask].index.tolist()\n",
    "\n",
    "        print(f\"Rows with GDR = 'Annat': {len(gdr_annat_indices)}\")\n",
    "\n",
    "        if len(gdr_annat_indices) == 0:\n",
    "            print(\"No GDR values marked as 'Annat' to impute\")\n",
    "            return\n",
    "\n",
    "        if 'GDR' not in self.models:\n",
    "            print(\"GDR model not trained. Using fallback method.\")\n",
    "            self._fallback_gdr_imputation(df, gdr_annat_indices)\n",
    "            return\n",
    "\n",
    "        try:\n",
    "            df_missing = df[gdr_annat_mask].copy()\n",
    "            features = self.feature_sets['GDR']\n",
    "            X_missing = df_missing[features]\n",
    "\n",
    "            if X_missing.isnull().any().any():\n",
    "                print(\"Warning: NaN values in required features for GDR prediction\")\n",
    "                valid_mask = ~X_missing.isnull().any(axis=1)\n",
    "                X_valid = X_missing[valid_mask]\n",
    "                valid_indices = X_valid.index.tolist()\n",
    "\n",
    "                if len(valid_indices) > 0:\n",
    "                    X_scaled = self.scalers['GDR'].transform(X_valid)\n",
    "                    predictions = self.models['GDR'].predict(X_scaled)\n",
    "\n",
    "                    for i, idx in enumerate(valid_indices):\n",
    "                        old_value = df.loc[idx, 'GDR']\n",
    "                        new_value = 'Kvinna' if predictions[i] == 0 else 'Man'\n",
    "                        df.loc[idx, 'GDR'] = new_value\n",
    "\n",
    "                        self._log_imputation(idx, 'GDR', old_value, new_value, 'Random Forest prediction')\n",
    "                        print(f\"  Row {idx}: GDR imputed from '{old_value}' to '{new_value}'\")\n",
    "\n",
    "                # Use fallback for remaining rows\n",
    "                remaining_indices = [idx for idx in gdr_annat_indices if idx not in valid_indices]\n",
    "                if remaining_indices:\n",
    "                    self._fallback_gdr_imputation(df, remaining_indices)\n",
    "            else:\n",
    "                X_scaled = self.scalers['GDR'].transform(X_missing)\n",
    "                predictions = self.models['GDR'].predict(X_scaled)\n",
    "\n",
    "                for i, idx in enumerate(gdr_annat_indices):\n",
    "                    old_value = df.loc[idx, 'GDR']\n",
    "                    new_value = 'Kvinna' if predictions[i] == 0 else 'Man'\n",
    "                    df.loc[idx, 'GDR'] = new_value\n",
    "\n",
    "                    self._log_imputation(idx, 'GDR', old_value, new_value, 'Random Forest prediction')\n",
    "                    print(f\"  Row {idx}: GDR imputed from '{old_value}' to '{new_value}'\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error in GDR Random Forest imputation: {e}\")\n",
    "            print(\"Using fallback method...\")\n",
    "            self._fallback_gdr_imputation(df, gdr_annat_indices)\n",
    "\n",
    "    def _fallback_gdr_imputation(self, df, indices):\n",
    "        \"\"\"Fallback method for GDR imputation using majority class.\"\"\"\n",
    "        print(f\"Using fallback (majority class) for {len(indices)} rows:\")\n",
    "\n",
    "        for idx in indices:\n",
    "            old_value = df.loc[idx, 'GDR']\n",
    "            new_value = 'Kvinna'  # Majority class\n",
    "            df.loc[idx, 'GDR'] = new_value\n",
    "\n",
    "            self._log_imputation(idx, 'GDR', old_value, new_value, 'Majority class (fallback)')\n",
    "            print(f\"  Row {idx}: GDR changed from '{old_value}' to '{new_value}' (fallback)\")\n",
    "\n",
    "    def _log_imputation(self, row_idx, column, old_value, new_value, method):\n",
    "        \"\"\"Log an imputation change.\"\"\"\n",
    "        self.imputation_log.append({\n",
    "            'Row': row_idx,\n",
    "            'Column': column,\n",
    "            'Before': old_value,\n",
    "            'After': new_value,\n",
    "            'Method': method\n",
    "        })\n",
    "\n",
    "    def _save_imputation_results(self, df_imputation):\n",
    "        \"\"\"Save imputation results and generate summary.\"\"\"\n",
    "        print(f\"\\n\\n\" + \"=\"*60)\n",
    "        print(\"IMPUTATION SUMMARY\")\n",
    "        print(\"=\"*60)\n",
    "\n",
    "        if self.imputation_log:\n",
    "            print(f\"Total imputed cells: {len(self.imputation_log)}\")\n",
    "\n",
    "            # Summary by column\n",
    "            column_counts = Counter([change['Column'] for change in self.imputation_log])\n",
    "            print(f\"\\nSummary by column:\")\n",
    "            for col, count in column_counts.items():\n",
    "                print(f\"  {col}: {count} imputations\")\n",
    "\n",
    "            # Save imputation log\n",
    "            imputation_df = pd.DataFrame(self.imputation_log)\n",
    "            imputation_df.to_excel(f'{timestamp}_imputation-log.xlsx', index=False)\n",
    "            print(f\"\\nImputation log saved as: imputation-log.xlsx\")\n",
    "\n",
    "            # Detailed log\n",
    "            print(f\"\\nDetailed imputation log:\")\n",
    "            for i, change in enumerate(self.imputation_log, 1):\n",
    "                print(f\"{i:2d}. Row {change['Row']:3d}, Column {change['Column']:3s}: \"\n",
    "                      f\"{str(change['Before']):10s} → {str(change['After']):10s} \"\n",
    "                      f\"({change['Method']})\")\n",
    "        else:\n",
    "            print(\"No cells were imputed\")\n",
    "\n",
    "        # Summary of imputation tracking columns\n",
    "        print(f\"\\nImputation tracking summary:\")\n",
    "        print(f\"  STU_IMPUTED column: {df_imputation['STU_IMPUTED'].sum()} rows marked as imputed\")\n",
    "        print(f\"  GDR_IMPUTED column: {df_imputation['GDR_IMPUTED'].sum()} rows marked as imputed\")\n",
    "\n",
    "        # Convert GDR to numeric before saving (but keep tracking columns as is)\n",
    "        df_final = df_imputation.copy()\n",
    "        df_final['GDR'] = df_final['GDR'].map({'Kvinna': 0, 'Man': 1})\n",
    "\n",
    "        # Save final dataset with imputation tracking\n",
    "        output_filename = f'{timestamp}_survey-teacher-data-with-imputation.xlsx'\n",
    "        df_final.to_excel(output_filename, index=False)\n",
    "        print(f\"\\nDataset with imputation tracking saved as: {output_filename}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function.\"\"\"\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Initialize processor\n",
    "    processor = SurveyDataProcessor(\n",
    "        'DataPreprocessor/Enkät om gymnasielärares syn på språkmodeller i undervisningen (Svar).xlsx'\n",
    "    )\n",
    "\n",
    "    # Load and prepare data\n",
    "    processor.load_and_prepare_data()\n",
    "\n",
    "    # Analyze missing values\n",
    "    processor.analyze_missing_values()\n",
    "\n",
    "    # Prepare training data\n",
    "    processor.prepare_training_data()\n",
    "\n",
    "    # Train models\n",
    "    print(f\"\\n\" + \"=\"*60)\n",
    "    print(\"MODEL TRAINING\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    stu_model, stu_scaler, stu_features = processor.train_model('STU', ['Nej', 'Ja'])\n",
    "    gdr_model, gdr_scaler, gdr_features = processor.train_model('GDR', ['Kvinna', 'Man'])\n",
    "\n",
    "    print(f\"STU important features: {stu_features}\")\n",
    "    print(f\"GDR important features: {gdr_features}\")\n",
    "\n",
    "    # Perform imputation\n",
    "    final_df = processor.perform_imputation()\n",
    "\n",
    "    # Final summary\n",
    "    end_time = time.time()\n",
    "    print(f\"\\nAnalysis completed!\")\n",
    "    print(f\"Execution time: {round(end_time - start_time, 2)} seconds\")\n",
    "    print(f\"Clean dataset size (for training): {len(processor.df_clean)} rows\")\n",
    "    print(f\"Final dataset size (with imputation): {len(final_df)} rows\")\n",
    "    print(f\"Number of imputed cells: {len(processor.imputation_log)}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preparing survey data...\n",
      "Data loaded: 223 rows\n",
      "\n",
      "============================================================\n",
      "MISSING VALUES ANALYSIS\n",
      "============================================================\n",
      "Total rows with missing values: 3\n",
      "\n",
      "Details of rows with missing values:\n",
      "Row 0: missing values in columns ['STU']\n",
      "Row 1: missing values in columns ['STU']\n",
      "Row 135: missing values in columns ['STU']\n",
      "\n",
      "Gender distribution:\n",
      "  Man: 121\n",
      "  Kvinna: 101\n",
      "  Annat: 1\n",
      "Clean dataset for training: 219 rows\n",
      "\n",
      "============================================================\n",
      "MODEL TRAINING\n",
      "============================================================\n",
      "\n",
      "Training model for STU...\n",
      "Model accuracy: 0.750\n",
      "Confusion matrix saved as: Tabell_1_Konfusionsmatris_STU.png\n",
      "\n",
      "Training model for GDR...\n",
      "Model accuracy: 0.591\n",
      "Confusion matrix saved as: Tabell_2_Konfusionsmatris_GDR.png\n",
      "STU important features: ['BI2', 'BI3', 'BI1', 'FC2', 'SI2', 'COUNTY', 'AGE', 'PE1']\n",
      "GDR important features: ['SI1', 'COUNTY', 'AGE', 'SI3', 'EE3', 'SI2', 'FC3', 'EE2']\n",
      "\n",
      "============================================================\n",
      "DETAILED IMPUTATION TRACKING\n",
      "============================================================\n",
      "Initialized imputation tracking columns:\n",
      "  STU values to be imputed: 3\n",
      "  GDR values to be imputed: 1\n",
      "\n",
      "STU IMPUTATION:\n",
      "----------------------------------------\n",
      "Rows with missing STU values: 3\n",
      "  Row 0: STU imputed from nan to 1.0\n",
      "  Row 1: STU imputed from nan to 1.0\n",
      "  Row 135: STU imputed from nan to 0.0\n",
      "\n",
      "\n",
      "GDR IMPUTATION WITH RANDOM FOREST:\n",
      "----------------------------------------\n",
      "Rows with GDR = 'Annat': 1\n",
      "  Row 109: GDR imputed from 'Annat' to 'Man'\n",
      "\n",
      "\n",
      "============================================================\n",
      "IMPUTATION SUMMARY\n",
      "============================================================\n",
      "Total imputed cells: 4\n",
      "\n",
      "Summary by column:\n",
      "  STU: 3 imputations\n",
      "  GDR: 1 imputations\n",
      "\n",
      "Imputation log saved as: imputation-log.xlsx\n",
      "\n",
      "Detailed imputation log:\n",
      " 1. Row   0, Column STU: nan        → 1.0        (Model prediction)\n",
      " 2. Row   1, Column STU: nan        → 1.0        (Model prediction)\n",
      " 3. Row 135, Column STU: nan        → 0.0        (Model prediction)\n",
      " 4. Row 109, Column GDR: Annat      → Man        (Random Forest prediction)\n",
      "\n",
      "Imputation tracking summary:\n",
      "  STU_IMPUTED column: 3 rows marked as imputed\n",
      "  GDR_IMPUTED column: 1 rows marked as imputed\n",
      "\n",
      "Dataset with imputation tracking saved as: 2025-06-01_12-24-18_survey-teacher-data-with-imputation.xlsx\n",
      "\n",
      "Analysis completed!\n",
      "Execution time: 2.0 seconds\n",
      "Clean dataset size (for training): 219 rows\n",
      "Final dataset size (with imputation): 223 rows\n",
      "Number of imputed cells: 4\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-31T15:02:48.598425Z",
     "start_time": "2025-05-31T15:02:48.578311Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "8807bdbdf996abb4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "fd7f2b3dc325d8ec"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
